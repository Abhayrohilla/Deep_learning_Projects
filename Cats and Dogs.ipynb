{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abecffb0-e811-47b3-87ae-85da42ae419f",
   "metadata": {},
   "source": [
    "Import the libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26e1d89d-2d84-4d46-956a-db0cbf759198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\abhay\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.preprocessing.image import ImageDataGenerator,load_img\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803bfec3-0989-445b-8082-9c752d75b9b9",
   "metadata": {},
   "source": [
    " Define image properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b06d57f3-7be9-4aff-ac0d-a977c32be5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image_Width=128\n",
    "Image_Height=128\n",
    "Image_Size=(Image_Width,Image_Height)\n",
    "Image_Channels=3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015f441a-040d-4102-98b2-0ca8d1c3ce1c",
   "metadata": {},
   "source": [
    "Prepare dataset for training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a588483-21e4-4f2c-bc5b-e7cf69e65105",
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames=os.listdir(r\"C:\\Users\\abhay\\OneDrive\\Desktop\\train\\train\")\n",
    "\n",
    "categories=[]\n",
    "for f_name in filenames:\n",
    "    category=f_name.split('.')[0]\n",
    "    if category=='dog':\n",
    "        categories.append(1)\n",
    "    else:\n",
    "        categories.append(0)\n",
    "\n",
    "df=pd.DataFrame({\n",
    "    'filename':filenames,\n",
    "    'category':categories\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e7a1ad-deea-4fd0-a631-7e7a142c08af",
   "metadata": {},
   "source": [
    "Create the neural net model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e78a9c3-8272-4b7d-9c59-e620d5038e1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\abhay\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\abhay\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\normalization\\batch_normalization.py:979: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\abhay\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D,MaxPooling2D,\\\n",
    "     Dropout,Flatten,Dense,Activation,\\\n",
    "     BatchNormalization\n",
    "\n",
    "model=Sequential()\n",
    "\n",
    "model.add(Conv2D(32,(3,3),activation='relu',input_shape=(Image_Width,Image_Height,Image_Channels)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(64,(3,3),activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(128,(3,3),activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512,activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(2,activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "  optimizer='rmsprop',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4d78bf-8958-4b7a-b1bd-1a20a1d599eb",
   "metadata": {},
   "source": [
    "Analyzing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a81bdbe-df1f-4b59-8bec-ca8709214210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 126, 126, 32)      896       \n",
      "                                                                 \n",
      " batch_normalization (Batch  (None, 126, 126, 32)      128       \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 63, 63, 32)        0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 63, 63, 32)        0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 61, 61, 64)        18496     \n",
      "                                                                 \n",
      " batch_normalization_1 (Bat  (None, 61, 61, 64)        256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 30, 30, 64)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 30, 30, 64)        0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 28, 28, 128)       73856     \n",
      "                                                                 \n",
      " batch_normalization_2 (Bat  (None, 28, 28, 128)       512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPoolin  (None, 14, 14, 128)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 14, 14, 128)       0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 25088)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               12845568  \n",
      "                                                                 \n",
      " batch_normalization_3 (Bat  (None, 512)               2048      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 2)                 1026      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 12942786 (49.37 MB)\n",
      "Trainable params: 12941314 (49.37 MB)\n",
      "Non-trainable params: 1472 (5.75 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36e2cac-5b21-46d6-b502-e6fcefaa5934",
   "metadata": {},
   "source": [
    "Define callbacks and learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f620c34-c943-40d7-9cb6-5e45484180db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "earlystop = EarlyStopping(patience = 10)\n",
    "learning_rate_reduction = ReduceLROnPlateau(monitor = 'val_acc',patience = 2,verbose = 1,factor = 0.5,min_lr = 0.00001)\n",
    "callbacks = [earlystop,learning_rate_reduction]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1b5c36-c2cb-4b26-b26a-b4bbb1ff9dd6",
   "metadata": {},
   "source": [
    "Manage data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53830ee7-f653-4baa-99a1-7f80c944f145",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"category\"] = df[\"category\"].replace({0:'cat',1:'dog'})\n",
    "train_df,validate_df = train_test_split(df,test_size=0.20,\n",
    "  random_state=42)\n",
    "\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "validate_df = validate_df.reset_index(drop=True)\n",
    "\n",
    "total_train=train_df.shape[0]\n",
    "total_validate=validate_df.shape[0]\n",
    "batch_size=15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbbd851-b57b-4f91-bd50-80aeadad1c29",
   "metadata": {},
   "source": [
    "Training and validation data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1031d11c-e633-4908-a2ab-4fab5b110913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20000 validated image filenames belonging to 2 classes.\n",
      "Found 5000 validated image filenames belonging to 2 classes.\n",
      "Found 0 validated image filenames belonging to 0 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abhay\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\preprocessing\\image.py:1137: UserWarning: Found 20000 invalid image filename(s) in x_col=\"filename\". These filename(s) will be ignored.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(rotation_range=15,\n",
    "                                rescale=1./255,\n",
    "                                shear_range=0.1,\n",
    "                                zoom_range=0.2,\n",
    "                                horizontal_flip=True,\n",
    "                                width_shift_range=0.1,\n",
    "                                height_shift_range=0.1\n",
    "                                )\n",
    "\n",
    "train_generator = train_datagen.flow_from_dataframe(train_df,\n",
    "                                                 r\"C:\\Users\\abhay\\OneDrive\\Desktop\\train\\train\",x_col='filename',y_col='category',\n",
    "                                                 target_size=Image_Size,\n",
    "                                                 class_mode='categorical',\n",
    "                                                 batch_size=batch_size)\n",
    "\n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    "validation_generator = validation_datagen.flow_from_dataframe(\n",
    "    validate_df, \n",
    "    r\"C:\\Users\\abhay\\OneDrive\\Desktop\\train\\train\", \n",
    "    x_col='filename',\n",
    "    y_col='category',\n",
    "    target_size=Image_Size,\n",
    "    class_mode='categorical',\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "test_datagen = ImageDataGenerator(rotation_range=15,\n",
    "                                rescale=1./255,\n",
    "                                shear_range=0.1,\n",
    "                                zoom_range=0.2,\n",
    "                                horizontal_flip=True,\n",
    "                                width_shift_range=0.1,\n",
    "                                height_shift_range=0.1)\n",
    "\n",
    "test_generator = train_datagen.flow_from_dataframe(train_df,\n",
    "                                                 r\"C:\\Users\\abhay\\OneDrive\\Desktop\\test1\\test1\",x_col='filename',y_col='category',\n",
    "                                                 target_size=Image_Size,\n",
    "                                                 class_mode='categorical',\n",
    "                                                 batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87eb6997-48b6-41e3-8948-a5b939bd3689",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e4418a3b-5f51-4613-97a2-5562df405828",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abhay\\AppData\\Local\\Temp\\ipykernel_28516\\2528005629.py:2: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  history = model.fit_generator(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "WARNING:tensorflow:From C:\\Users\\abhay\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\abhay\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "1333/1333 [==============================] - ETA: 0s - loss: 0.7457 - accuracy: 0.6277WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "1333/1333 [==============================] - 1362s 1s/step - loss: 0.7457 - accuracy: 0.6277 - val_loss: 0.5603 - val_accuracy: 0.7067 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "1333/1333 [==============================] - ETA: 0s - loss: 0.5580 - accuracy: 0.7217WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "1333/1333 [==============================] - 750s 563ms/step - loss: 0.5580 - accuracy: 0.7217 - val_loss: 0.7448 - val_accuracy: 0.6533 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "1333/1333 [==============================] - ETA: 0s - loss: 0.5045 - accuracy: 0.7590WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "1333/1333 [==============================] - 627s 470ms/step - loss: 0.5045 - accuracy: 0.7590 - val_loss: 0.6427 - val_accuracy: 0.7465 - lr: 0.0010\n",
      "Epoch 4/10\n",
      "1333/1333 [==============================] - ETA: 0s - loss: 0.4719 - accuracy: 0.7811WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "1333/1333 [==============================] - 620s 465ms/step - loss: 0.4719 - accuracy: 0.7811 - val_loss: 0.4009 - val_accuracy: 0.8226 - lr: 0.0010\n",
      "Epoch 5/10\n",
      "1333/1333 [==============================] - ETA: 0s - loss: 0.4332 - accuracy: 0.8014WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "1333/1333 [==============================] - 818s 614ms/step - loss: 0.4332 - accuracy: 0.8014 - val_loss: 0.4277 - val_accuracy: 0.8126 - lr: 0.0010\n",
      "Epoch 6/10\n",
      "1333/1333 [==============================] - ETA: 0s - loss: 0.4204 - accuracy: 0.8115WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "1333/1333 [==============================] - 707s 531ms/step - loss: 0.4204 - accuracy: 0.8115 - val_loss: 0.4207 - val_accuracy: 0.8120 - lr: 0.0010\n",
      "Epoch 7/10\n",
      "1333/1333 [==============================] - ETA: 0s - loss: 0.4030 - accuracy: 0.8198WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "1333/1333 [==============================] - 627s 470ms/step - loss: 0.4030 - accuracy: 0.8198 - val_loss: 0.4301 - val_accuracy: 0.8256 - lr: 0.0010\n",
      "Epoch 8/10\n",
      "1333/1333 [==============================] - ETA: 0s - loss: 0.3897 - accuracy: 0.8261WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "1333/1333 [==============================] - 620s 465ms/step - loss: 0.3897 - accuracy: 0.8261 - val_loss: 0.4902 - val_accuracy: 0.8178 - lr: 0.0010\n",
      "Epoch 9/10\n",
      "1333/1333 [==============================] - ETA: 0s - loss: 0.3814 - accuracy: 0.8312WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "1333/1333 [==============================] - 614s 460ms/step - loss: 0.3814 - accuracy: 0.8312 - val_loss: 0.3763 - val_accuracy: 0.8196 - lr: 0.0010\n",
      "Epoch 10/10\n",
      "1333/1333 [==============================] - ETA: 0s - loss: 0.3732 - accuracy: 0.8366WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "1333/1333 [==============================] - 611s 458ms/step - loss: 0.3732 - accuracy: 0.8366 - val_loss: 0.3573 - val_accuracy: 0.8438 - lr: 0.0010\n"
     ]
    }
   ],
   "source": [
    "epochs=10\n",
    "history = model.fit_generator(\n",
    "    train_generator, \n",
    "    epochs=epochs,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=total_validate//batch_size,\n",
    "    steps_per_epoch=total_train//batch_size,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df68d31-af15-4f6a-9def-1ab8572d072c",
   "metadata": {},
   "source": [
    "Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac3ae83b-f65c-44ee-9b34-8b74ff5bf2b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abhay\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "model.save(\"model1_catsVSdogs_10epoch.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c3bffa-1a4f-4bde-84e6-cf72fad5e5ec",
   "metadata": {},
   "source": [
    " Test your model performance on custom data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b2c1bb0e-c63e-416c-8099-0009de94e16b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n",
      "1 dog\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Assuming Image_Size is defined\n",
    "Image_Size = (128, 128)\n",
    "\n",
    "results = {\n",
    "    0: 'cat',\n",
    "    1: 'dog'\n",
    "}\n",
    "\n",
    "# Load the saved model\n",
    "model = load_model(\"model1_catsVSdogs_10epoch.h5\")\n",
    "\n",
    "# Load and preprocess the image\n",
    "im = Image.open(r\"C:\\Users\\abhay\\OneDrive\\Desktop\\test1\\test1\\2.jpg\")\n",
    "im = im.resize(Image_Size)\n",
    "im = np.expand_dims(im, axis=0)\n",
    "im = np.array(im)\n",
    "im = im / 255.0\n",
    "\n",
    "# Predict the class of the image\n",
    "pred = model.predict(im)\n",
    "pred_class = np.argmax(pred, axis=1)[0]\n",
    "\n",
    "print(pred_class, results[pred_class])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549c2104-bd17-4f21-8b12-d28d5236a3b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
